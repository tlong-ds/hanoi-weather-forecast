{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f663b4",
   "metadata": {},
   "source": [
    "## Task7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461ddb7",
   "metadata": {},
   "source": [
    "#### In your opinion, can we do better? I propose that your team try all ML models that you know and give us the model with the best possible precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5fe556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8000 | Test size: 2000\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Common Setup (run once)\n",
    "# - Loads NLTK twitter_samples\n",
    "# - Fixed split: train = [:4000], test = [4000:] for each class\n",
    "# - Minimal tweet cleaning\n",
    "# - Produces: train_x_clean, test_x_clean, train_y, test_y\n",
    "# =========================\n",
    "import re, string, numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# If not cached locally, keep these lines enabled\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1) Load dataset\n",
    "pos = twitter_samples.strings('positive_tweets.json')\n",
    "neg = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# 2) Fixed split per requirement\n",
    "train_pos, test_pos = pos[:4000], pos[4000:]\n",
    "train_neg, test_neg = neg[:4000], neg[4000:]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x  = test_pos + test_neg\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))  # 1=Positive, 0=Negative\n",
    "test_y  = np.append(np.ones(len(test_pos)),  np.zeros(len(test_neg)))\n",
    "\n",
    "# 3) Minimal, robust cleaning for short tweets\n",
    "STOP = set(stopwords.words('english'))\n",
    "def clean_tweet(t: str) -> str:\n",
    "    t = re.sub(r'http\\S+', '', t)   # remove URLs\n",
    "    t = re.sub(r'@\\w+', '', t)      # remove @mentions\n",
    "    t = t.lower()\n",
    "    t = t.translate(str.maketrans('', '', string.punctuation))\n",
    "    toks = [w for w in t.split() if w not in STOP]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# 4) Preprocess once and reuse\n",
    "train_x_clean = [clean_tweet(t) for t in train_x]\n",
    "test_x_clean  = [clean_tweet(t) for t in test_x]\n",
    "\n",
    "print(f\"Train size: {len(train_x_clean)} | Test size: {len(test_x_clean)}\")\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    \"\"\"Prints precision for Positive=1 and Negative=0, plus full report and confusion matrix.\"\"\"\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Precision (Positive=1):\", precision_score(y_true, y_pred, pos_label=1))\n",
    "    print(\"Precision (Negative=0):\", precision_score(y_true, y_pred, pos_label=0))\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "    print(\"Confusion matrix [[TN, FP],[FN, TP]]:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b9751",
   "metadata": {},
   "source": [
    "##### * There are some ML models that my team knows and that can give us better precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43fde4",
   "metadata": {},
   "source": [
    "1, Linear SVM (LinearSVC, TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ce9640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LinearSVC (TF-IDF) ===\n",
      "Precision (Positive=1): 0.7551020408163265\n",
      "Precision (Negative=0): 0.7221702525724977\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.77      0.75      1000\n",
      "    Positive       0.76      0.70      0.73      1000\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.74      0.74      0.74      2000\n",
      "weighted avg       0.74      0.74      0.74      2000\n",
      "\n",
      "Confusion matrix [[TN, FP],[FN, TP]]:\n",
      "[[772 228]\n",
      " [297 703]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_tr = tfidf.fit_transform(train_x_clean)\n",
    "X_te = tfidf.transform(test_x_clean)\n",
    "\n",
    "svm_lin = LinearSVC(C=1.0, random_state=RANDOM_STATE)\n",
    "svm_lin.fit(X_tr, train_y)\n",
    "\n",
    "y_pred = svm_lin.predict(X_te)\n",
    "evaluate_model(\"LinearSVC (TF-IDF)\", test_y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bc1bc",
   "metadata": {},
   "source": [
    "2, RBF SVM (SVC with RBF, TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93018a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVC RBF (TF-IDF) ===\n",
      "Precision (Positive=1): 0.7694013303769401\n",
      "Precision (Negative=0): 0.7213114754098361\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.79      0.76      1000\n",
      "    Positive       0.77      0.69      0.73      1000\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.75      0.74      0.74      2000\n",
      "weighted avg       0.75      0.74      0.74      2000\n",
      "\n",
      "Confusion matrix [[TN, FP],[FN, TP]]:\n",
      "[[792 208]\n",
      " [306 694]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_tr = tfidf.fit_transform(train_x_clean)\n",
    "X_te = tfidf.transform(test_x_clean)\n",
    "\n",
    "svm_rbf = SVC(kernel=\"rbf\", C=2.0, gamma=\"scale\", random_state=RANDOM_STATE)\n",
    "svm_rbf.fit(X_tr, train_y)\n",
    "\n",
    "y_pred = svm_rbf.predict(X_te)\n",
    "evaluate_model(\"SVC RBF (TF-IDF)\", test_y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859b82e",
   "metadata": {},
   "source": [
    "3, Multinomial Naive Bayes (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6907c70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MultinomialNB (TF-IDF) ===\n",
      "Precision (Positive=1): 0.7788018433179723\n",
      "Precision (Negative=0): 0.7137809187279152\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.71      0.81      0.76      1000\n",
      "    Positive       0.78      0.68      0.72      1000\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.75      0.74      0.74      2000\n",
      "weighted avg       0.75      0.74      0.74      2000\n",
      "\n",
      "Confusion matrix [[TN, FP],[FN, TP]]:\n",
      "[[808 192]\n",
      " [324 676]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_tr = tfidf.fit_transform(train_x_clean)\n",
    "X_te = tfidf.transform(test_x_clean)\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.5)  # try 0.3–1.0 for tuning\n",
    "mnb.fit(X_tr, train_y)\n",
    "\n",
    "y_pred = mnb.predict(X_te)\n",
    "evaluate_model(\"MultinomialNB (TF-IDF)\", test_y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d3323",
   "metadata": {},
   "source": [
    "4, Bernoulli Naive Bayes (Binary Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9a1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BernoulliNB (Binary BoW) ===\n",
      "Precision (Positive=1): 0.799265605875153\n",
      "Precision (Negative=0): 0.7066779374471682\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.71      0.84      0.77      1000\n",
      "    Positive       0.80      0.65      0.72      1000\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.75      0.74      0.74      2000\n",
      "weighted avg       0.75      0.74      0.74      2000\n",
      "\n",
      "Confusion matrix [[TN, FP],[FN, TP]]:\n",
      "[[836 164]\n",
      " [347 653]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bin_vec = CountVectorizer(binary=True, ngram_range=(1,2), min_df=2)\n",
    "X_tr = bin_vec.fit_transform(train_x_clean)\n",
    "X_te = bin_vec.transform(test_x_clean)\n",
    "\n",
    "bnb = BernoulliNB(alpha=0.5)\n",
    "bnb.fit(X_tr, train_y)\n",
    "\n",
    "y_pred = bnb.predict(X_te)\n",
    "evaluate_model(\"BernoulliNB (Binary BoW)\", test_y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36dc8d7",
   "metadata": {},
   "source": [
    "6, XGBoost (TF-IDF)(Requires: pip install xgboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "987e8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost (TF-IDF) ===\n",
      "Precision (Positive=1): 0.7958579881656804\n",
      "Precision (Negative=0): 0.6510574018126888\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.86      0.74      1000\n",
      "    Positive       0.80      0.54      0.64      1000\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.72      0.70      0.69      2000\n",
      "weighted avg       0.72      0.70      0.69      2000\n",
      "\n",
      "Confusion matrix [[TN, FP],[FN, TP]]:\n",
      "[[862 138]\n",
      " [462 538]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_tr = tfidf.fit_transform(train_x_clean)\n",
    "X_te = tfidf.transform(test_x_clean)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb.fit(X_tr, train_y)\n",
    "\n",
    "y_pred = xgb.predict(X_te)\n",
    "evaluate_model(\"XGBoost (TF-IDF)\", test_y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be995cde",
   "metadata": {},
   "source": [
    "#### Conclusion ..........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e71e64",
   "metadata": {},
   "source": [
    "## Task 8\n",
    "#### We are in 2025 right now, so use some Virtual Assistant such as ChatGPT (or better call API of LLM model) as the benchmark and find a way to run the test set in your course with ChatGPT to determine the sentiment. What is your conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d6fa2",
   "metadata": {},
   "source": [
    "*(values depend on actual run, typically high since the dataset is clean and balanced).*\n",
    "\n",
    "### 1. ChatGPT API (LLM-based Classifier)\n",
    "- **Setup**: For each tweet, a prompt was sent to GPT-4o-mini with instruction:  \n",
    "*“Classify the sentiment of this tweet as Positive or Negative”*.\n",
    "- **No training** required; the model directly predicts labels.\n",
    "- **Results (20 tweets)**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b928a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.95      0.93        20\n",
      "    Positive       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "test (20 positive + 20 negative)\n",
    "test_pos = all_positive_tweets[4000:4020]\n",
    "test_neg = all_negative_tweets[4000:4020]\n",
    "test_x = test_pos + test_neg\n",
    "test_y = [1]*len(test_pos) + [0]*len(test_neg)   # 1 = Positive, 0 = Negative\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "def classify_with_chatgpt(sentence):\n",
    "    prompt = f\"Classify the sentiment of this tweet as Positive or Negative:\\n\\nTweet: {sentence}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    label = response.choices[0].message.content.strip().lower()\n",
    "    return 1 if \"positive\" in label else 0\n",
    "\n",
    "y_pred = [classify_with_chatgpt(s) for s in test_x]\n",
    "\n",
    "print(classification_report(test_y, y_pred, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae75c4",
   "metadata": {},
   "source": [
    "*(values vary by run; generally comparable to Logistic Regression, sometimes slightly better on informal or tricky tweets).*\n",
    "\n",
    "### Key Observations\n",
    "- **Accuracy**: Both models perform well on this small balanced test set.  \n",
    "- **Efficiency**:\n",
    "- Logistic Regression is **instantaneous** and free after training.\n",
    "- ChatGPT API requires **20 separate API calls** (slower, costs tokens).\n",
    "- **Generalization**:\n",
    "- Logistic Regression relies on word frequencies; struggles with slang, emojis, sarcasm.\n",
    "- ChatGPT leverages pre-training on massive corpora, so it often handles informal text better.\n",
    "- **Reproducibility**:\n",
    "- Logistic Regression is fully deterministic with a fixed random seed.\n",
    "- ChatGPT output may vary, though setting `temperature=0` reduces randomness.\n",
    "\n",
    "### Conclusion\n",
    "Logistic Regression provides a **strong, fast, reproducible baseline**.  \n",
    "ChatGPT API delivers **comparable or superior accuracy** on nuanced tweets,  \n",
    "but at the cost of **time and API usage**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06db50",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
