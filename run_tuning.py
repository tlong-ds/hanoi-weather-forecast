# ================== run_tuning_DIRECT.py ==================
# <<< THAY Äá»”I >>> PhiÃªn báº£n nÃ y Ä‘Æ°á»£c sá»­a Ä‘á»ƒ cháº¡y 5 quy trÃ¬nh tuning riÃªng biá»‡t
# cho 5 bá»™ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c feature selection riÃªng
# ========================================================

import warnings
warnings.filterwarnings("ignore")

import optuna
import numpy as np
import pandas as pd # <<< THAY Äá»”I >>> Cáº§n thÃªm pandas Ä‘á»ƒ load data
import os # <<< THAY Äá»”I >>> Cáº§n thÃªm os
from clearml import Task, Logger
from model_helper import DEVICE # Giáº£ sá»­ DEVICE váº«n Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong model_helper
from sklearn.ensemble import RandomForestRegressor
# <<< THAY Äá»”I >>> KHÃ”NG Cáº¦N MultiOutputRegressor ná»¯a
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

# =============== BÆ¯á»šC 1: KHá»I Táº O CLEARML TASK ===============
task = Task.init(
    project_name="HanoiTemp_Forecast",
    task_name="Optuna_Tuning_Direct_Strategy (5 Models)" # <<< THAY Äá»”I >>> TÃªn task má»›i
)

# <<< THAY Äá»”I >>> Biáº¿n toÃ n cá»¥c Ä‘á»ƒ lÆ°u trá»¯ data cho má»—i trial
# ChÃºng sáº½ Ä‘Æ°á»£c cáº­p nháº­t bÃªn trong vÃ²ng láº·p á»Ÿ main
current_X_train, current_y_train = None, None
current_X_dev, current_y_dev = None, None

# <<< THAY Äá»”I >>> Äá»‹nh nghÄ©a sá»‘ ngÃ y dá»± bÃ¡o
N_STEPS_AHEAD = 5

# =============== BÆ¯á»šC 2: Táº¢I Dá»® LIá»†U (Äá»ŠNH NGHÄ¨A HÃ€M Má»šI) ===============
def load_data_for_day(day_step):
    """
    Táº£i bá»™ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ riÃªng cho ngÃ y t+{day_step}
    """
    day_str = f"t_{day_step}"
    data_dir = f'processed_data/target_{day_str}'
    
    print(f"\nLoading data from: {data_dir}")
    
    try:
        X_train = pd.read_csv(os.path.join(data_dir, f'X_train_{day_str}.csv'), index_col=0)
        y_train = pd.read_csv(os.path.join(data_dir, f'y_train_{day_str}.csv'), index_col=0)
        
        X_dev = pd.read_csv(os.path.join(data_dir, f'X_dev_{day_str}.csv'), index_col=0)
        y_dev = pd.read_csv(os.path.join(data_dir, f'y_dev_{day_str}.csv'), index_col=0)
        
        # <<< THAY Äá»”I >>> Quan trá»ng: y_train/y_dev giá» chá»‰ lÃ  1 cá»™t.
        # Cáº§n .values.ravel() Ä‘á»ƒ biáº¿n nÃ³ thÃ nh máº£ng 1D mÃ  model mong Ä‘á»£i
        return X_train, y_train.values.ravel(), X_dev, y_dev.values.ravel()
    
    except FileNotFoundError as e:
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u cho {day_str}. Báº¡n Ä‘Ã£ cháº¡y preprocessing.py chÆ°a?")
        print(e)
        return None, None, None, None


# =============== BÆ¯á»šC 3: Äá»ŠNH NGHÄ¨A OBJECTIVE FUNCTION (OPTUNA) ===============
# <<< THAY Äá»”I >>> HÃ m objective giá» sáº½ dÃ¹ng data toÃ n cá»¥c
def objective(trial):
    model_name = trial.suggest_categorical(
        "model_name",
        ["Random Forest", "XGBoost", "LightGBM", "CatBoost"]
    )

    # -------- RANDOM FOREST --------
    if model_name == "Random Forest":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 400, step=50),
            "max_depth": trial.suggest_int("max_depth", 6, 18),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 6),
            "random_state": 42,
            "n_jobs": -1,
        }
        # <<< THAY Äá»”I >>> KhÃ´ng cÃ²n MultiOutputRegressor
        model = RandomForestRegressor(**params)

    # -------- XGBOOST --------
    elif model_name == "XGBoost" and XGBRegressor is not None:
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "max_depth": trial.suggest_int("max_depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "tree_method": "hist",
            "device": "cuda" if str(DEVICE) == "cuda" else "cpu",
            "random_state": 42,
        }
        # <<< THAY Äá»”I >>> KhÃ´ng cÃ²n MultiOutputRegressor
        model = XGBRegressor(**params)

    # -------- LIGHTGBM --------
    elif model_name == "LightGBM" and LGBMRegressor is not None:
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "num_leaves": trial.suggest_int("num_leaves", 20, 60),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 5.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 5.0),
            "device": "gpu",
            "device_type": "cuda",
            "random_state": 42,
        }
        # <<< THAY Äá»”I >>> KhÃ´ng cÃ²n MultiOutputRegressor
        model = LGBMRegressor(**params)

    # -------- CATBOOST --------
    elif model_name == "CatBoost" and CatBoostRegressor is not None:
        params = {
            "iterations": trial.suggest_int("iterations", 200, 600, step=100),
            "depth": trial.suggest_int("depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1),
            "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1.0, 5.0),
            "bootstrap_type": "Bernoulli",
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "loss_function": "RMSE",
            "task_type": "GPU" if str(DEVICE) == "cuda" else "CPU",
            "verbose": 0,
            "random_state": 42,
        }
        # <<< THAY Äá»”I >>> KhÃ´ng cÃ²n MultiOutputRegressor
        model = CatBoostRegressor(**params)

    else:
        raise ValueError(f"âŒ Model {model_name} khÃ´ng kháº£ dá»¥ng hoáº·c chÆ°a Ä‘Æ°á»£c import.")

    # -------- TRAIN + DEV EVALUATION --------
    # <<< THAY Äá»”I >>> DÃ¹ng biáº¿n toÃ n cá»¥c Ä‘Ã£ Ä‘Æ°á»£c set trong vÃ²ng láº·p main
    global current_X_train, current_y_train, current_X_dev, current_y_dev
    
    model.fit(current_X_train, current_y_train)
    y_pred = model.predict(current_X_dev)

    # <<< THAY Äá»”I >>> y_dev giá» lÃ  máº£ng 1D, nÃªn viá»‡c tÃ­nh toÃ¡n váº«n giá»¯ nguyÃªn
    rmse = np.sqrt(mean_squared_error(current_y_dev, y_pred))
    mae = mean_absolute_error(current_y_dev, y_pred)
    mape = mean_absolute_percentage_error(current_y_dev, y_pred)

    # -------- Log ClearML --------
    # KhÃ´ng log scalar á»Ÿ Ä‘Ã¢y ná»¯a, sáº½ log tá»•ng káº¿t á»Ÿ main
    
    # print(f"Trial {trial.number}: {model_name}, RMSE={rmse:.4f}")
    return rmse  # minimize RMSE


# =============== BÆ¯á»šC 4: CHáº Y OPTUNA STUDY (TRONG VÃ’NG Láº¶P) ===============
if __name__ == "__main__":
    
    # <<< THAY Äá»”I >>> Táº¡o má»™t logger ClearML
    logger = Logger.current_logger()
    
    print(f"===== ğŸš€ Báº®T Äáº¦U 5 QUY TRÃŒNH TUNING (CHO t+1 Äáº¾N t+{N_STEPS_AHEAD}) =====")
    
    # <<< THAY Äá»”I >>> VÃ²ng láº·p chÃ­nh, cháº¡y 5 láº§n
    for day_step in range(1, N_STEPS_AHEAD + 1):
        day_str = f"t+{day_step}"
        print(f"\n{'='*70}")
        print(f"ğŸ¯ Báº®T Äáº¦U TUNING CHO NGÃ€Y: {day_str}")
        print(f"{'='*70}")
        
        # 1. Táº£i data cho ngÃ y nÃ y
        X_train, y_train, X_dev, y_dev = load_data_for_day(day_step)
        if X_train is None:
            continue # Bá» qua náº¿u khÃ´ng táº£i Ä‘Æ°á»£c
            
        # 2. GÃ¡n data vÃ o biáº¿n toÃ n cá»¥c Ä‘á»ƒ hÃ m objective tháº¥y
        current_X_train, current_y_train = X_train, y_train
        current_X_dev, current_y_dev = X_dev, y_dev

        print(f"âœ… Dá»¯ liá»‡u {day_str} train: {X_train.shape}, target: {y_train.shape}")
        print(f"âœ… Dá»¯ liá»‡u {day_str} dev: {X_dev.shape}, target: {y_dev.shape}")
        
        # 3. Táº¡o má»™t Study Má»šI cho ngÃ y nÃ y
        study = optuna.create_study(
            direction="minimize",
            study_name=f"Tuning_4Models_{day_str}"
        )
        
        # 4. Cháº¡y optimize
        study.optimize(objective, n_trials=60, show_progress_bar=True)

        # 5. Láº¥y káº¿t quáº£ tá»‘t nháº¥t cho ngÃ y nÃ y
        best_params = study.best_trial.params
        best_rmse = study.best_value
        best_model_name = best_params.get("model_name", "N/A")

        print(f"\n===== ğŸ¯ Tá»”NG Káº¾T CHO {day_str} =====")
        print(f"  Best Model: {best_model_name}")
        print(f"  Best RMSE: {best_rmse:.4f}")
        print(f"  Best Params: {best_params}")

        # 6. Log káº¿t quáº£ tá»‘t nháº¥t cho ngÃ y nÃ y lÃªn ClearML
        logger.report_scalar(
            title="Best RMSE per Day",
            series=f"{day_str}",
            value=best_rmse,
            iteration=day_step
        )
        logger.report_scalar(
            title="Best Model per Day",
            series=f"{day_str}",
            value=best_model_name, # Log tÃªn model
            iteration=day_step
        )
        logger.report_text(f"Best Params {day_str}: {best_params}")

    print("\nğŸ‰ğŸ‰ğŸ‰ HoÃ n táº¥t Cáº¢ 5 quy trÃ¬nh tuning & log lÃªn ClearML! ğŸ‰ğŸ‰ğŸ‰")