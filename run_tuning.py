# ================== run_tuning_DIRECT.py ==================
# PhiÃªn báº£n nÃ y Ä‘Ã£ sá»­a lá»—i tÃªn file (X_train_t1.csv)
# ========================================================

import warnings
warnings.filterwarnings("ignore")

import optuna
import numpy as np
import pandas as pd
import os
from clearml import Task, Logger
from model_helper import DEVICE 
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

# =============== BÆ¯á»šC 1: KHá»I Táº O CLEARML TASK ===============
task = Task.init(
    project_name="HanoiTemp_Forecast",
    task_name="Optuna_Tuning_Direct_Strategy (5 Models)"
)

# Biáº¿n toÃ n cá»¥c Ä‘á»ƒ lÆ°u trá»¯ data cho má»—i trial
current_X_train, current_y_train = None, None
current_X_dev, current_y_dev = None, None

N_STEPS_AHEAD = 5

# =============== BÆ¯á»šC 2: Táº¢I Dá»® LIá»†U (Äá»ŠNH NGHÄ¨A HÃ€M Má»šI) ===============
def load_data_for_day(day_step):
    """
    Táº£i bá»™ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ riÃªng cho ngÃ y t+{day_step}
    """
    # day_str dÃ¹ng cho tÃªn thÆ° má»¥c (VÃ Dá»¤: 't_1')
    day_str = f"t_{day_step}"
    data_dir = f'processed_data/target_{day_str}'
    
    print(f"\nLoading data from: {data_dir}")
    
    try:
        # <<< Sá»¬A Lá»–I TÃŠN FILE á» ÄÃ‚Y >>>
        # TÃªn file Ä‘Ãºng lÃ  f'X_train_t{day_step}.csv' (vÃ­ dá»¥: 'X_train_t1.csv')
        
        X_train_file = f'X_train_t{day_step}.csv'
        y_train_file = f'y_train_t{day_step}.csv'
        X_dev_file = f'X_dev_t{day_step}.csv'
        y_dev_file = f'y_dev_t{day_step}.csv'

        X_train = pd.read_csv(os.path.join(data_dir, X_train_file), index_col=0)
        y_train = pd.read_csv(os.path.join(data_dir, y_train_file), index_col=0)
        
        X_dev = pd.read_csv(os.path.join(data_dir, X_dev_file), index_col=0)
        y_dev = pd.read_csv(os.path.join(data_dir, y_dev_file), index_col=0)
        
        # Chuyá»ƒn y (1 cá»™t) thÃ nh máº£ng 1D
        return X_train, y_train.values.ravel(), X_dev, y_dev.values.ravel()
    
    except FileNotFoundError as e:
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file trong thÆ° má»¥c '{data_dir}'.")
        print(f"  Äáº£m báº£o báº¡n Ä‘Ã£ cháº¡y 'preprocessing.py' vÃ  file '{X_train_file}' tá»“n táº¡i.")
        print(f"  Lá»—i gá»‘c: {e}")
        return None, None, None, None

# =============== BÆ¯á»šC 3: Äá»ŠNH NGHÄ¨A OBJECTIVE FUNCTION (OPTUNA) ===============
def objective(trial):
    model_name = trial.suggest_categorical(
        "model_name",
        ["Random Forest", "XGBoost", "LightGBM", "CatBoost"]
    )

    # -------- RANDOM FOREST --------
    if model_name == "Random Forest":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 400, step=50),
            "max_depth": trial.suggest_int("max_depth", 6, 18),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 6),
            "random_state": 42,
            "n_jobs": -1, # DÃ¹ng CPU
        }
        model = RandomForestRegressor(**params)

    # -------- XGBOOST --------
    elif model_name == "XGBoost" and XGBRegressor is not None:
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "max_depth": trial.suggest_int("max_depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "tree_method": "hist",
            "device": "cuda" if str(DEVICE) == "cuda" else "cpu",
            "random_state": 42,
        }
        model = XGBRegressor(**params)

    # -------- LIGHTGBM --------
    elif model_name == "LightGBM" and LGBMRegressor is not None:
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "num_leaves": trial.suggest_int("num_leaves", 20, 60),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 5.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 5.0),
            "device_type": "cpu",  # Thay "cuda" thÃ nh "cpu"
            "n_jobs": -1,          # ThÃªm láº¡i n_jobs vÃ¬ giá» cháº¡y báº±ng CPU
            "random_state": 42,
        }
        model = LGBMRegressor(**params)

    # -------- CATBOOST --------
    elif model_name == "CatBoost" and CatBoostRegressor is not None:
        params = {
            "iterations": trial.suggest_int("iterations", 200, 600, step=100),
            "depth": trial.suggest_int("depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1),
            "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1.0, 5.0),
            "bootstrap_type": "Bernoulli",
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "loss_function": "RMSE",
            "task_type": "GPU" if str(DEVICE) == "cuda" else "CPU",
            "verbose": 0,
            "random_state": 42,
        }
        model = CatBoostRegressor(**params)

    else:
        raise ValueError(f"âŒ Model {model_name} khÃ´ng kháº£ dá»¥ng hoáº·c chÆ°a Ä‘Æ°á»£c import.")

    # -------- TRAIN + DEV EVALUATION --------
    global current_X_train, current_y_train, current_X_dev, current_y_dev
    
    model.fit(current_X_train, current_y_train)
    y_pred = model.predict(current_X_dev)

    rmse = np.sqrt(mean_squared_error(current_y_dev, y_pred))
    mae = mean_absolute_error(current_y_dev, y_pred)
    mape = mean_absolute_percentage_error(current_y_dev, y_pred)

    return rmse  # minimize RMSE


# =============== BÆ¯á»šC 4: CHáº Y OPTUNA STUDY (TRONG VÃ’NG Láº¶P) ===============
if __name__ == "__main__":
    
    logger = Logger.current_logger()
    
    print(f"===== ğŸš€ Báº®T Äáº¦U 5 QUY TRÃŒNH TUNING (CHO t+1 Äáº¾N t+{N_STEPS_AHEAD}) =====")
    
    for day_step in range(1, N_STEPS_AHEAD + 1):
        day_str = f"t+{day_step}"
        print(f"\n{'='*70}")
        print(f"ğŸ¯ Báº®T Äáº¦U TUNING CHO NGÃ€Y: {day_str}")
        print(f"{'='*70}")
        
        # 1. Táº£i data cho ngÃ y nÃ y
        X_train, y_train, X_dev, y_dev = load_data_for_day(day_step)
        if X_train is None:
            continue # Bá» qua náº¿u khÃ´ng táº£i Ä‘Æ°á»£c
            
        # 2. GÃ¡n data vÃ o biáº¿n toÃ n cá»¥c
        current_X_train, current_y_train = X_train, y_train
        current_X_dev, current_y_dev = X_dev, y_dev

        print(f"âœ… Dá»¯ liá»‡u {day_str} train: {X_train.shape}, target: {y_train.shape}")
        print(f"âœ… Dá»¯ liá»‡u {day_str} dev: {X_dev.shape}, target: {y_dev.shape}")
        
        # 3. Táº¡o má»™t Study Má»šI cho ngÃ y nÃ y
        study = optuna.create_study(
            direction="minimize",
            study_name=f"Tuning_4Models_{day_str}"
        )
        
        # 4. Cháº¡y optimize
        study.optimize(objective, n_trials=60, show_progress_bar=True)

        # 5. Láº¥y káº¿t quáº£ tá»‘t nháº¥t cho ngÃ y nÃ y
        best_params = study.best_trial.params
        best_rmse = study.best_value
        best_model_name = best_params.get("model_name", "N/A")

        print(f"\n===== ğŸ¯ Tá»”NG Káº¾T CHO {day_str} =====")
        print(f"  Best Model: {best_model_name}")
        print(f"  Best RMSE: {best_rmse:.4f}")
        print(f"  Best Params: {best_params}")

        # 6. Log káº¿t quáº£ tá»‘t nháº¥t cho ngÃ y nÃ y lÃªn ClearML
        logger.report_scalar(
            title="Best RMSE per Day",
            series=f"{day_str}",
            value=best_rmse,
            iteration=day_step
        )
        logger.report_scalar(
            title="Best Model per Day",
            series=f"{day_str}",
            value=best_model_name, # Log tÃªn model
            iteration=day_step
        )
        logger.report_text(f"Best Params {day_str}: {best_params}")

    print("\nğŸ‰ğŸ‰ğŸ‰ HoÃ n táº¥t Cáº¢ 5 quy trÃ¬nh tuning & log lÃªn ClearML! ğŸ‰ğŸ‰ğŸ‰")