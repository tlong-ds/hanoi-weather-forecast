# run_tuning.py
import optuna
from clearml import Task, Logger
from model_train import load_processed_data, train_models
from model_evaluate import load_data, calculate_metrics
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor


# =============== BÆ¯á»šC 1: KHá»I Táº O CLEARML TASK ===============
task = Task.init(
    project_name="HanoiTemp_Forecast",
    task_name="Optuna_Tuning_3Models"
)

# =============== BÆ¯á»šC 2: Táº¢I Dá»® LIá»†U ===============
X_train, y_train = load_processed_data()
X_dev, y_dev = load_data('dev')

if X_train is None or X_dev is None:
    raise FileNotFoundError("âŒ KhÃ´ng táº£i Ä‘Æ°á»£c dá»¯ liá»‡u. Kiá»ƒm tra thÆ° má»¥c processed_data.")


# =============== BÆ¯á»šC 3: Äá»ŠNH NGHÄ¨A HÃ€M CHO OPTUNA ===============
def objective(trial):
    # --- 3.1 Chá»n model ---
    model_name = trial.suggest_categorical(
        "model_name",
        ["Linear Regression", "Random Forest", "XGBoost (MultiOutput)"]
    )

    # --- 3.2 Gá»£i Ã½ hyperparameters tÃ¹y theo model ---
    if model_name == "Linear Regression":
        # LinearRegression khÃ´ng cÃ³ hyperparam phá»©c táº¡p, nÃªn chá»‰ cáº§n khá»Ÿi táº¡o Ä‘Æ¡n giáº£n
        model_instance = LinearRegression()

    elif model_name == "Random Forest":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 400, step=50),
            "max_depth": trial.suggest_int("max_depth", 6, 18),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 6),
            "random_state": 42,
            "n_jobs": -1
        }
        model_instance = RandomForestRegressor(**params)

    else:  # XGBoost (MultiOutput)
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "max_depth": trial.suggest_int("max_depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "random_state": 42,
            "n_jobs": -1
        }
        model_instance = MultiOutputRegressor(XGBRegressor(**params))

    # --- 3.3 Huáº¥n luyá»‡n model ---
    trained_models = train_models(
        X_train,
        y_train,
        models_to_train={model_name: model_instance}
    )
    model = trained_models[model_name]

    # --- 3.4 Dá»± Ä‘oÃ¡n trÃªn táº­p dev ---
    y_pred = model.predict(X_dev)

    # --- 3.5 ÄÃ¡nh giÃ¡ káº¿t quáº£ ---
    metrics, _ = calculate_metrics(y_dev, y_pred, model_name=model_name)
    avg_rmse = metrics["Average_RMSE"]

    # --- 3.6 Ghi log lÃªn ClearML ---
    Logger.current_logger().report_scalar(
        title="Validation RMSE",
        series=model_name,
        value=avg_rmse,
        iteration=trial.number
    )

    print(f"âœ… Trial {trial.number} ({model_name}) -> RMSE trung bÃ¬nh = {avg_rmse:.4f}")
    return avg_rmse


# =============== BÆ¯á»šC 4: Táº O VÃ€ CHáº Y OPTUNA STUDY ===============
study = optuna.create_study(
    direction="minimize",
    study_name="LR_RF_XGB_Tuning"
)
study.optimize(objective, n_trials=30)

# =============== BÆ¯á»šC 5: BÃO CÃO Káº¾T QUáº¢ ===============
print("\n===== ğŸ¯ Tá»”NG Káº¾T CUá»˜C THI =====")
print("Best trial params:")
print(study.best_trial.params)
print(f"Lowest RMSE: {study.best_value:.4f}")

Logger.current_logger().report_text(f"Best Trial Params: {study.best_trial.params}")
Logger.current_logger().report_scalar(
    title="Best RMSE",
    series="Optuna",
    value=study.best_value,
    iteration=study.best_trial.number
)
