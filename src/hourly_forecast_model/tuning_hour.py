# ================== run_tuning_hourly.py ==================
# Tuning cho b√†i to√°n d·ª± b√°o nhi·ªát ƒë·ªô hourly (t+1h..t+24h)
# D·ªØ li·ªáu l·∫•y t·ª´: data_processing_hourly/
# C·∫•u tr√∫c d·ª±a tr√™n: run_tuning_DIRECT.py
#
# PHI√äN B·∫¢N ƒê√É S·ª¨A:
# 1. Ch·ªâ load data 1 l·∫ßn (ngo√†i v√≤ng l·∫∑p)
# 2. B·ªè bi·∫øn global, d√πng lambda
# 3. Th√™m `log=True` cho learning_rate
# 4. C·∫£i thi·ªán ClearML logging
# 5. Ki·ªÉm tra model kh·∫£ d·ª•ng
# ==========================================================

import warnings
warnings.filterwarnings("ignore")

import os
import numpy as np
import pandas as pd
import optuna

from clearml import Task, Logger

# N·∫øu b·∫°n c√≥ s·∫µn model_helper.DEVICE th√¨ gi·ªØ d√≤ng sau,
# c√≤n kh√¥ng th√¨ comment l·∫°i v√† d√πng DEVICE = "cpu"
try:
    from model_helper import DEVICE
except ImportError:
    DEVICE = "cpu"

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

try:
    from xgboost import XGBRegressor
except Exception:
    XGBRegressor = None

try:
    from lightgbm import LGBMRegressor
except Exception:
    LGBMRegressor = None

try:
    from catboost import CatBoostRegressor
except Exception:
    CatBoostRegressor = None

# =============== CONSTANTS ===============
DATA_DIR = "data_processing_hourly"
X_TRAIN_FILE = "X_train_transformed_hourly.csv"
Y_TRAIN_FILE = "y_train_hourly.csv"
X_DEV_FILE   = "X_dev_transformed_hourly.csv"
Y_DEV_FILE   = "y_dev_hourly.csv"

# 24 horizons: t+1h..t+24h
N_STEPS_AHEAD = 24


# =============== B∆Ø·ªöC 1: KH·ªûI T·∫†O CLEARML TASK ===============
task = Task.init(
    project_name="HanoiTemp_Forecast_Hourly",
    task_name="Optuna_Tuning_Hourly_MultiStep (4 Models)"
)

# =============== B∆Ø·ªöC 2: T·∫¢I D·ªÆ LI·ªÜU (ƒê√É B·ªé) ===============
# H√†m load_data_for_horizon ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p v√†o main()
# ƒë·ªÉ tr√°nh ƒë·ªçc file 24 l·∫ßn.


# =============== B∆Ø·ªöC 3: ƒê·ªäNH NGHƒ®A OBJECTIVE FUNCTION (OPTUNA) ===============
# Th√™m tham s·ªë (X_train, y_train, X_dev, y_dev) ƒë·ªÉ tr√°nh d√πng global
def objective(trial, X_train, y_train, X_dev, y_dev):
    
    # [FIX 2] T·ª± ƒë·ªông build danh s√°ch model ƒë√£ c√†i
    available_models = ["Random Forest"]
    if XGBRegressor is not None:
        available_models.append("XGBoost")
    if LGBMRegressor is not None:
        available_models.append("LightGBM")
    if CatBoostRegressor is not None:
        available_models.append("CatBoost")

    model_name = trial.suggest_categorical(
        "model_name",
        available_models # Ch·ªâ ch·ªçn t·ª´ c√°c model kh·∫£ d·ª•ng
    )

    # -------- RANDOM FOREST --------
    if model_name == "Random Forest":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 400, step=50),
            "max_depth": trial.suggest_int("max_depth", 6, 18),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 6),
            "random_state": 42,
            "n_jobs": -1,
        }
        model = RandomForestRegressor(**params)

    # -------- XGBOOST --------
    elif model_name == "XGBoost":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "max_depth": trial.suggest_int("max_depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True), # [FIX 5]
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "tree_method": "hist",
            "device": "cuda" if str(DEVICE) == "cuda" else "cpu",
            "random_state": 42,
        }
        model = XGBRegressor(**params)

    # -------- LIGHTGBM --------
    elif model_name == "LightGBM":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "num_leaves": trial.suggest_int("num_leaves", 20, 60),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True), # [FIX 5]
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 5.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 5.0),
            "device_type": "cpu",
            "n_jobs": -1,
            "random_state": 42,
        }
        model = LGBMRegressor(**params)

    # -------- CATBOOST --------
    elif model_name == "CatBoost":
        params = {
            "iterations": trial.suggest_int("iterations", 200, 600, step=100),
            "depth": trial.suggest_int("depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1, log=True), # [FIX 5]
            "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1.0, 5.0),
            "bootstrap_type": "Bernoulli",
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "loss_function": "RMSE",
            "task_type": "GPU" if str(DEVICE) == "cuda" else "CPU",
            "verbose": 0,
            "random_state": 42,
        }
        model = CatBoostRegressor(**params)

    # -------- TRAIN + EVAL TR√äN DEV --------
    # [FIX 3] Kh√¥ng c·∫ßn global, d√πng tham s·ªë truy·ªÅn v√†o
    model.fit(X_train, y_train)
    y_pred = model.predict(X_dev)

    rmse = np.sqrt(mean_squared_error(y_dev, y_pred))
    mae  = mean_absolute_error(y_dev, y_pred)
    mape = mean_absolute_percentage_error(y_dev, y_pred)

    # (tu·ª≥ ch·ªçn) log th√™m v√†o trial ƒë·ªÉ xem ph√¢n b·ªë
    trial.set_user_attr("mae", float(mae))
    trial.set_user_attr("mape", float(mape))

    return rmse  # minimize RMSE


# =============== B∆Ø·ªöC 4: CH·∫†Y OPTUNA STUDY THEO HORIZON ===============
if __name__ == "__main__":

    logger = Logger.current_logger()

    # [FIX 1] T·∫£i d·ªØ li·ªáu 1 L·∫¶N DUY NH·∫§T
    print("üöÄ ƒêang t·∫£i d·ªØ li·ªáu X, y (DataFrame)...")
    try:
        X_train_path = os.path.join(DATA_DIR, X_TRAIN_FILE)
        y_train_path = os.path.join(DATA_DIR, Y_TRAIN_FILE)
        X_dev_path   = os.path.join(DATA_DIR, X_DEV_FILE)
        y_dev_path   = os.path.join(DATA_DIR, Y_DEV_FILE)

        X_train = pd.read_csv(X_train_path, index_col=0)
        y_train_df = pd.read_csv(y_train_path, index_col=0)
        X_dev = pd.read_csv(X_dev_path, index_col=0)
        y_dev_df = pd.read_csv(y_dev_path, index_col=0)
        
        print(f"  T·∫£i th√†nh c√¥ng X_train: {X_train.shape}, y_train_df: {y_train_df.shape}")
        print(f"  T·∫£i th√†nh c√¥ng X_dev:   {X_dev.shape},   y_dev_df:   {y_dev_df.shape}")

    except Exception as e:
        print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu ban ƒë·∫ßu. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        print(f"  Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n: {os.path.abspath(DATA_DIR)}")
        print(f"  L·ªói g·ªëc: {e}")
        exit() # Tho√°t n·∫øu kh√¥ng load ƒë∆∞·ª£c file

    print(f"===== üöÄ B·∫ÆT ƒê·∫¶U TUNING CHO 24 HORIZONS (t+1h .. t+{N_STEPS_AHEAD}h) =====")

    for h_step in range(1, N_STEPS_AHEAD + 1):
        horizon_str = f"t+{h_step}h"
        target_col = f"target_temp_t+{h_step}h" # ƒê·∫£m b·∫£o t√™n c·ªôt n√†y ch√≠nh x√°c

        print(f"\n{'='*80}")
        print(f"üéØ B·∫ÆT ƒê·∫¶U TUNING CHO HORIZON: {horizon_str} (C·ªôt: {target_col})")
        print(f"{'='*80}")

        # 1. [FIX 1] L·∫•y d·ªØ li·ªáu y cho horizon n√†y (kh√¥ng ƒë·ªçc file)
        if target_col not in y_train_df.columns:
            print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y c·ªôt {target_col} trong y_train_df. B·ªè qua horizon n√†y.")
            continue
        
        y_train = y_train_df[target_col].values.ravel()
        y_dev   = y_dev_df[target_col].values.ravel()

        # 2. [FIX 3] Kh√¥ng c·∫ßn g√°n v√†o bi·∫øn global

        # 3. T·∫°o m·ªôt Study ri√™ng cho m·ªói horizon
        study = optuna.create_study(
            direction="minimize",
            study_name=f"Tuning_4Models_{horizon_str}"
        )

        # 4. [FIX 3] Ch·∫°y optimize d√πng lambda ƒë·ªÉ truy·ªÅn data
        study.optimize(
            lambda trial: objective(trial, X_train, y_train, X_dev, y_dev), 
            n_trials=60, 
            show_progress_bar=True
        )

        # 5. L·∫•y k·∫øt qu·∫£ t·ªët nh·∫•t
        best_params = study.best_trial.params
        best_rmse   = study.best_value
        best_model_name = best_params.get("model_name", "N/A")

        print(f"\n===== üéØ T·ªîNG K·∫æT CHO {horizon_str} =====")
        print(f"  Best Model: {best_model_name}")
        print(f"  Best RMSE:  {best_rmse:.4f}")
        print(f"  Best Params:")
        for k, v in best_params.items():
            print(f"    - {k}: {v}")

        # 6. Log k·∫øt qu·∫£ l√™n ClearML
        
        # [FIX 4] Log RMSE v√†o 1 bi·ªÉu ƒë·ªì duy nh·∫•t
        logger.report_scalar(
            title="Best RMSE per Horizon",  # T√™n bi·ªÉu ƒë·ªì
            series="RMSE",                  # T√™n ƒë∆∞·ªùng line
            value=best_rmse,                # Gi√° tr·ªã (tr·ª•c Y)
            iteration=h_step                # Horizon (tr·ª•c X)
        )

        # Log model name
        logger.report_text(
            f"[{horizon_str}] Best Model: {best_model_name}",
            level="INFO"
        )

        # Log full params
        logger.report_text(
            f"[{horizon_str}] Best Params: {best_params}"
        )

    print("\nüéâüéâüéâ Ho√†n t·∫•t tuning cho to√†n b·ªô 24 horizons & log l√™n ClearML! üéâüéâüéâ")