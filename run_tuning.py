# ================== run_tuning_DIRECT.py ==================
# Phi√™n b·∫£n n√†y ƒë√£ s·ª≠a l·ªói t√™n file (X_train_t1.csv)
# ========================================================

import warnings
warnings.filterwarnings("ignore")

import optuna
import numpy as np
import pandas as pd
import os
from clearml import Task, Logger
from model_helper import DEVICE 
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

# =============== B∆Ø·ªöC 1: KH·ªûI T·∫†O CLEARML TASK ===============
task = Task.init(
    project_name="HanoiTemp_Forecast",
    task_name="Optuna_Tuning_Direct_Strategy (5 Models)"
)

# Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u tr·ªØ data cho m·ªói trial
current_X_train, current_y_train = None, None
current_X_dev, current_y_dev = None, None

N_STEPS_AHEAD = 5

# =============== B∆Ø·ªöC 2: T·∫¢I D·ªÆ LI·ªÜU (ƒê·ªäNH NGHƒ®A H√ÄM M·ªöI) ===============
def load_data_for_day(day_step):
    """
    T·∫£i b·ªô d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ri√™ng cho ng√†y t+{day_step}
    """
    # day_str d√πng cho t√™n th∆∞ m·ª•c (V√ç D·ª§: 't_1')
    day_str = f"t_{day_step}"
    data_dir = f'processed_data/target_{day_str}'
    
    print(f"\nLoading data from: {data_dir}")
    
    try:
        # <<< S·ª¨A L·ªñI T√äN FILE ·ªû ƒê√ÇY >>>
        # T√™n file ƒë√∫ng l√† f'X_train_t{day_step}.csv' (v√≠ d·ª•: 'X_train_t1.csv')
        
        X_train_file = f'X_train_t{day_step}.csv'
        y_train_file = f'y_train_t{day_step}.csv'
        X_dev_file = f'X_dev_t{day_step}.csv'
        y_dev_file = f'y_dev_t{day_step}.csv'

        X_train = pd.read_csv(os.path.join(data_dir, X_train_file), index_col=0)
        y_train = pd.read_csv(os.path.join(data_dir, y_train_file), index_col=0)
        
        X_dev = pd.read_csv(os.path.join(data_dir, X_dev_file), index_col=0)
        y_dev = pd.read_csv(os.path.join(data_dir, y_dev_file), index_col=0)
        
        # Chuy·ªÉn y (1 c·ªôt) th√†nh m·∫£ng 1D
        return X_train, y_train.values.ravel(), X_dev, y_dev.values.ravel()
    
    except FileNotFoundError as e:
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file trong th∆∞ m·ª•c '{data_dir}'.")
        print(f"  ƒê·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y 'preprocessing.py' v√† file '{X_train_file}' t·ªìn t·∫°i.")
        print(f"  L·ªói g·ªëc: {e}")
        return None, None, None, None

# =============== B∆Ø·ªöC 3: ƒê·ªäNH NGHƒ®A OBJECTIVE FUNCTION (OPTUNA) ===============
def objective(trial):
    model_name = trial.suggest_categorical(
        "model_name",
        ["Random Forest", "XGBoost", "LightGBM", "CatBoost"]
    )

    # -------- RANDOM FOREST --------
    if model_name == "Random Forest":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 400, step=50),
            "max_depth": trial.suggest_int("max_depth", 6, 18),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 6),
            "random_state": 42,
            "n_jobs": -1, # D√πng CPU
        }
        model = RandomForestRegressor(**params)

    # -------- XGBOOST --------
    elif model_name == "XGBoost" and XGBRegressor is not None:
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "max_depth": trial.suggest_int("max_depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "tree_method": "hist",
            "device": "cuda" if str(DEVICE) == "cuda" else "cpu",
            "random_state": 42,
        }
        model = XGBRegressor(**params)

    # -------- LIGHTGBM --------
    elif model_name == "LightGBM" and LGBMRegressor is not None:
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 200, 600, step=100),
            "num_leaves": trial.suggest_int("num_leaves", 20, 60),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 5.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 5.0),
            # "device": "gpu",  <--- B·ªé D√íNG N√ÄY
            "device_type": "cuda", # <--- GI·ªÆ L·∫†I D√íNG N√ÄY
            "random_state": 42,
        }
        model = LGBMRegressor(**params)

    # -------- CATBOOST --------
    elif model_name == "CatBoost" and CatBoostRegressor is not None:
        params = {
            "iterations": trial.suggest_int("iterations", 200, 600, step=100),
            "depth": trial.suggest_int("depth", 4, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1),
            "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1.0, 5.0),
            "bootstrap_type": "Bernoulli",
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "loss_function": "RMSE",
            "task_type": "GPU" if str(DEVICE) == "cuda" else "CPU",
            "verbose": 0,
            "random_state": 42,
        }
        model = CatBoostRegressor(**params)

    else:
        raise ValueError(f"‚ùå Model {model_name} kh√¥ng kh·∫£ d·ª•ng ho·∫∑c ch∆∞a ƒë∆∞·ª£c import.")

    # -------- TRAIN + DEV EVALUATION --------
    global current_X_train, current_y_train, current_X_dev, current_y_dev
    
    model.fit(current_X_train, current_y_train)
    y_pred = model.predict(current_X_dev)

    rmse = np.sqrt(mean_squared_error(current_y_dev, y_pred))
    mae = mean_absolute_error(current_y_dev, y_pred)
    mape = mean_absolute_percentage_error(current_y_dev, y_pred)

    return rmse  # minimize RMSE


# =============== B∆Ø·ªöC 4: CH·∫†Y OPTUNA STUDY (TRONG V√íNG L·∫∂P) ===============
if __name__ == "__main__":
    
    logger = Logger.current_logger()
    
    print(f"===== üöÄ B·∫ÆT ƒê·∫¶U 5 QUY TR√åNH TUNING (CHO t+1 ƒê·∫æN t+{N_STEPS_AHEAD}) =====")
    
    for day_step in range(1, N_STEPS_AHEAD + 1):
        day_str = f"t+{day_step}"
        print(f"\n{'='*70}")
        print(f"üéØ B·∫ÆT ƒê·∫¶U TUNING CHO NG√ÄY: {day_str}")
        print(f"{'='*70}")
        
        # 1. T·∫£i data cho ng√†y n√†y
        X_train, y_train, X_dev, y_dev = load_data_for_day(day_step)
        if X_train is None:
            continue # B·ªè qua n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c
            
        # 2. G√°n data v√†o bi·∫øn to√†n c·ª•c
        current_X_train, current_y_train = X_train, y_train
        current_X_dev, current_y_dev = X_dev, y_dev

        print(f"‚úÖ D·ªØ li·ªáu {day_str} train: {X_train.shape}, target: {y_train.shape}")
        print(f"‚úÖ D·ªØ li·ªáu {day_str} dev: {X_dev.shape}, target: {y_dev.shape}")
        
        # 3. T·∫°o m·ªôt Study M·ªöI cho ng√†y n√†y
        study = optuna.create_study(
            direction="minimize",
            study_name=f"Tuning_4Models_{day_str}"
        )
        
        # 4. Ch·∫°y optimize
        study.optimize(objective, n_trials=60, show_progress_bar=True)

        # 5. L·∫•y k·∫øt qu·∫£ t·ªët nh·∫•t cho ng√†y n√†y
        best_params = study.best_trial.params
        best_rmse = study.best_value
        best_model_name = best_params.get("model_name", "N/A")

        print(f"\n===== üéØ T·ªîNG K·∫æT CHO {day_str} =====")
        print(f"  Best Model: {best_model_name}")
        print(f"  Best RMSE: {best_rmse:.4f}")
        print(f"  Best Params: {best_params}")

        # 6. Log k·∫øt qu·∫£ t·ªët nh·∫•t cho ng√†y n√†y l√™n ClearML
        logger.report_scalar(
            title="Best RMSE per Day",
            series=f"{day_str}",
            value=best_rmse,
            iteration=day_step
        )
        logger.report_scalar(
            title="Best Model per Day",
            series=f"{day_str}",
            value=best_model_name, # Log t√™n model
            iteration=day_step
        )
        logger.report_text(f"Best Params {day_str}: {best_params}")

    print("\nüéâüéâüéâ Ho√†n t·∫•t C·∫¢ 5 quy tr√¨nh tuning & log l√™n ClearML! üéâüéâüéâ")